# Il problema di Shannon
Quello che voleva fare shannon è di risolvere un grosso problema divindendolo in più sottoproblemi.

Il problema della trasmissione di dati su un canale aflitto da rumore si scompone in due parti, quello della codifica dell'informazione dalla sorgente e come gestire il rumore ed il canale.

Riassumendo: 
1) Massimizzare la quantità di informazione qualsiasi volta si utilizza il canale
2) Minimizzare il numero di errori

### Come risolvere il problema
Procediamo per step.
Per risolvere il primo punto abbiamo bisogno di un metodo per codificare i messaggi. Il codice in questione deve presentare certe caratteristiche che poi ci permetteranno di decodificarlo.

In termini matematici  ci serve un **INSIEME** finito di simboli con cui possiamo rappresentare un messaggio *x*. Una volta che abbiamo un modo per rappresentare il messaggio della sorgente abbiamo bisogno di un codice per la codifica, idealmnete che riduca il numero di simboli necessari, appunto massimizziamo la quantità di informazioni per uso del canale, la traduciamo in termini matematici come una funzione.
$$ c: X ->  (0... d-1 )^+$$
Un codice d-iario dove d è un numero e nel caso specifico di d=2 avremo un codice composto da 0 e 1. Il + di "esponente" indica una qualsiasi combinazione/concatenazione dei simboli del codice. Quindi c(a) può ad esempio esser rappresentato come 0, come 001 o 1111 e così via.

Definiamo inoltre una funzione che calcola la lunghezza delle parole del codice.
Nell'esempio di a -> 001 la funzione Lc(a) = 3.

Per ottenere un codice ottimale avremo bisogno di definire la probabilità che una determinata parola sia prodotta dalla sorgente.

### Distribuzione della probabilità di un messaggio
Per ogni parola da x1 a xn possiamo definire la probabilità come eventi indipendenti  e quindi la probabilità sarà intesa come la frequenza di ciascuna parola.
Questo nella realtà non avviene, soprattutto se consideriamo il linguaggio in cui data una lettera possiamo prevedere la prossima con maggiori informazioni ed infatti il codice zip utilizza questa conoscenza per rendere più efficente la propria compressione.

Definita la probabilità per ogni lettera possiamo quindi definire il valore atteso come la sommattoria della lunghezza di ogni singola parola per la probabilità che questa venga emessa. Noi cerchiamo il valore minimo possibile per il nostro codice.

### Il problema reale
I problemi reali che incontriamo sono quelli di decodifica di un dato codice. Non è detto che il nostro codice "minimale" sia decodificabile, in termini tecnici si dice che non sia singolare. In termini di funzioni matematiche ci servirà una funzione iniettiva.

Inoltre non è detto che la concatenazione del nostro codice sia anch'esso singolare.
Quello che stiamo cercando è un determinato codice C (dove C grande è il codice concatenato) che sia **univocamente decodificabile** o UD in breve.

Esistono algoritmi che permettono di verificare se un determinato codice sia UD, tra cui il sardinas Patterson ma non saranno trattati ora.
Uno dei problemi che possono verificarsi sui codici UD è di non essere immediamente decodificabili data una certa ambiguità iniziale che può essere risolta una volta ottenuto tutto il messaggio.
Ci servono quindi un'altro tipo di codici che sono gli istantanei, ovvero immediatamente decodificabili.
Per essere istantaneo una delle proprietà del codice è di non avere parole del codice che siano prefissi di altri e deve valere per ogni parola.

### Teorema: se un codice è istantaneo allora significa che è anche ud
La dimostrazione segue per assurdo.
Dimostriamo che se un un codice non è UD allora non è neanche istantaneo

Per definizione se non è UD allora avremo che almeno una parola sarà  codificata in un modo identico ad un'altra. Chiamo rispettivamente x1 e x2 le due parole (non nulle e non uguali ).

C(x1) = C(x2)
Esistono due opzioni :
1) la parola x1 e x2 sono codificate e una delle due è parte dell'altra e termina con un carattere nullo
2) Le parole x1 e x2 sono codificate in modo tale che la loro parte iniziale sia la stessa ma divergono fino alla loro fine.

In entrambi i casi arriviamo a dimostrare un assurdo..